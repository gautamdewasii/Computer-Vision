{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f20ffc-57b2-4bb0-af04-4e662142095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time  # for identify the current time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284470f7-f17a-479d-9f43-1ca67eb2813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting with default camera 0\n",
    "cap=cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3345a6-64c1-41b7-be10-b2c316f63fdc",
   "metadata": {},
   "source": [
    "#### Showing frames on window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88567f5a-c61d-4242-be91-487ff8cf4682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# untile sucess ( in this case infinite untile we close the terminal\n",
    "while True:\n",
    "    # reading image and suceess status from camera using read()\n",
    "    success, img = cap.read()\n",
    "    # displaying that img(frame ) on window using opencv imshow()\n",
    "    cv2.imshow(\"Image capture\", img)\n",
    "    # 1 frame generate after 1milliosecond\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa42a83-3433-465f-aa7c-ab1f9ec8cdec",
   "metadata": {},
   "source": [
    "#### Creating mediapipe objects and processing each frame using them, and generating each frame hands landmarks positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3230f-30bb-4e08-8258-133ad87060b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this contains all information about hands , and its detection \n",
    "mp_hands = mp.solutions.hands\n",
    "# object of Hands class , default variables like no of hands =2 , etc\n",
    "hands = mp_hands.Hands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a41e0-e2da-4bbf-955f-cbd759dc99c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # mediapipe works only on RGB color image\n",
    "    # converting into rgb\n",
    "    img_rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    # process() used to detect hands in an each frame\n",
    "    # now, result contains all hands landmarks details\n",
    "    # NOTE :- each image(frame) landmarks data are in List() formate inside results.\n",
    "    # and these results are of similiar to dictionary type ( mediapipe object )\n",
    "    results = hands.process(img_rgb)     # result type is List( mediapipe objects similiar to dict in it) = [ {}, {}, ... , {} ] \n",
    "    #### check this out\n",
    "    #print(results.multi_hand_landmarks)\n",
    "    #print(type(results.multi_hand_landmarks))\n",
    "    \n",
    "    # if hand detecting \n",
    "    if results.multi_hand_landmarks:\n",
    "        # accessing each individual landmark points one by one from result LIST()\n",
    "        # \"hand_landmarks\" is of type :- <class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList'> \n",
    "        # similiar to dictionary objects\n",
    "        # NOTE :- at each iteration \"hands_landmarks\" added landmark details into its dictionary type object\n",
    "        for hands_landmarks in results.multi_hand_landmarks:\n",
    "            ##### check this out\n",
    "            #print(hands_landmarks)\n",
    "            #print(type(hands_landmarks))\n",
    "\n",
    "            #### check this \n",
    "            #<class 'google.protobuf.internal.containers.RepeatedCompositeFieldContainer'> = list()\n",
    "            # all \"hands_landmarks\" items into list() type formate, so, easy to identify indexes of each landmarks\n",
    "            #print(hands_landmarks.landmark)\n",
    "            #print(type(hands_landmarks.landmark))\n",
    "\n",
    "            # \"id\" is of integer type\n",
    "            # \"lm\" is <class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmark'> type or similiar to dictionary\n",
    "            for id,lm in enumerate(hands_landmarks.landmark):\n",
    "                #print(id,lm)\n",
    "                #print(type(id),type(lm))\n",
    "                \n",
    "    cv2.imshow(\"Image capture\", img)\n",
    "\n",
    "    # capture only one frame untile we can't response or create event \n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef142156-3b6e-45f8-9b90-bbf032244a1c",
   "metadata": {},
   "source": [
    "#### Converting landmarks positions values into pixels formate and displaying \"filled circle\" on each landmark points:-\n",
    "USING cv2.circle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd925af-c18c-4999-9930-0ad9f1e68fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img_rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)    \n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hands_landmarks in results.multi_hand_landmarks:\n",
    "            for id,lm in enumerate(hands_landmarks.landmark):\n",
    "\n",
    "                # img.shape = (height, width, channel(RGB) )\n",
    "                height, width, channel = img.shape\n",
    "                # converting fractional x and y axis points to pixels on the basis of image shape\n",
    "                # 'cx' and 'cy' are the positions of landmark from the central top left of the image\n",
    "                cx, cy = int(lm.x*width), int(lm.y*height)\n",
    "                # displaying each frame's 21 landmarks along with their 'id' number\n",
    "                print(id, cx, cy)\n",
    "                # calling cv2 method circle()\n",
    "                # img :- actual image\n",
    "                # (cx, cy) :- landmarks positions on the image\n",
    "                #5 :- radius of the circle\n",
    "                # (255,0,255) color of the circle\n",
    "                # cv2.FILLED :- circle is filled with boder color\n",
    "                cv2.circle(img, (cx, cy), 5, (255, 0, 255), cv2.FILLED\n",
    "                \n",
    "    cv2.imshow(\"Image capture\", img)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d68220c-5ead-4389-873c-8b6016f6e3ba",
   "metadata": {},
   "source": [
    "#### Displaying landmark points and their connecting line with the help of mediapipe drawing utilities:-\n",
    "#### And displaying frame-per-second on the camera window using cv2 method\n",
    "using mediapipe drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd58c9-1f1e-4299-89a3-3aacb6e4ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediapipe drawing utilities\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# for calculating frame per second\n",
    "current_time=0\n",
    "previews_time=0\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img_rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)    \n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hands_landmarks in results.multi_hand_landmarks:\n",
    "            # displaying landmarks and connecting lines on frame using mediapipe utils\n",
    "            # mp_hands.HAND_CONNECTIONS :- used to draw lines between landmark points\n",
    "            mp_draw.draw_landmarks(img,hands_landmarks,mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # time.time() returns time in second from 1970 ( first epoch )\n",
    "    # to get your device time epoch , \n",
    "    #obj = time.gmtime(0) \n",
    "    #epoch = time.asctime(obj) \n",
    "    current_time = time.time()\n",
    "    # time required to generate one frame\n",
    "    frame_per_second = 1/(current_time - previews_time)\n",
    "    previews_time = current_time\n",
    "\n",
    "    # for displaying frame per second on window\n",
    "    # frame_per_second :- firstly converting it from fraction to intger , then into string\n",
    "    # 10,60 is the text display position\n",
    "    #  cv2.FONT_HERSHEY_PLAIN , font style of the text\n",
    "    # 2 is the scaler ( higher means larger size )\n",
    "    # 3 :- thickness of the text\n",
    "    cv2.putText(img,str(int(frame_per_second)), ( 10, 60), cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 255), 3  )\n",
    "    \n",
    "    cv2.imshow(\"Image capture\", img)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f72fac-e089-400d-b705-b668f3bacde4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
